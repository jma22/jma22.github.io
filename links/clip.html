<!DOCTYPE html>
<html>
<head>
  <link rel="stylesheet" href="./links.css">
</head>
<body>
    <h1>The CLIP Octopus</h1>
    <hr>

    <h2>tldr;</h2>
        <p class="text">A class project that trains an RL robot that can change color and shape to mimic a concept/object we give it.</p>

        <center>
                <img src = "../media/clip/ani.gif" width="20%" height="20%" vertical-align= "top">        
        </img>  
                <p>Animation of "snake" color and position changing</p> 
        </center>
        <center>
            <img src = "../media/clip/results.png" width="75%" height="75%" >        
        </img>  
        <p>Resulting images</p>
    </center><br>
    <hr>



    <h2>Paper</h2>
        <a href = "../media/clip/The_CLIP_Octopus.pdf"><p class = "text" > linklonk</p></a>
    
        <h2>Presentation</h2>
        <a href = "https://docs.google.com/presentation/d/15GEWRWkJ4F4jJfs5TBza4_rlK5g1hn91Y0SiXAEZtjU/edit?usp=sharing"><p class = "text" > linklonk</p></a>
    <h2> Summary </h2>
    <h3>Model training</h3>
        <ol>
            <li>CLIP is a model that measures similarity between images and prompts</li>
            <li>Used CLIP to convert natural language into a reward signal to train an agent to maximize it's rendered image and the CLIP prompt</li>
            <li>This leads to simple task specification without manual fine tuning</li>
        </ol>
    <h3>Takeaway</h3>
    <p class="text">This approach does not only outperform conventional pixel matching approaches in terms of convergence, but it also provides the agent with the flexibility to explore the whole manifold of shapes that matches the prompt.</p>
    <hr>
    <h2> Key Figures</h2>
    <center>
        <img src = "../media/clip/graphs.png" width="75%" height="75%">        
        <img>   
    </center><br>
    <center>
        <img src = "../media/clip/pipe.png" width="75%" height="75%">        
        <img>   
    </center><br>
 
    <center>
        <img src = "../media/clip/results2.png" width="75%" height="75%">        
        <img>   
    </center><br>
    

    <h2>Abstract</h2>
    <p class="text">Task specification is hard. Current methods in robot learning require either hand-
        tuned reward functions for reinforcement learning or expert-generated demonstra-
        tions for offline learning—neither of which are easily specified by an end user.
        One pathway towards developing more human intuitive methods of goal specifi-
        cation is to leverage the capabilities of CLIP, a large-scale vision-language model
        pre-trained on millions of natural examples. In this work, we aim to utilize the
        translation capabilities of CLIP to relate language, a natural avenue for human task
        specification, to images, a form that all robot states can be expressed as. We use
        cosine similarity between the two as a common language, or proxy reward, to train
        a “Snake” robot to maneuver itself into resembling various visual concepts defined
        by an end user. Specifically, we explore how to best utilize a potentially rich and
        dense reward signal generated automatically by CLIP in order to best design a
        visually expressive robot capable of taking on the form of a specification expressed
        by a human using natural language alone. We explore various prompting strategies,
        losses, reward densities, and robot configurations and find that CLIP can indeed be
        used as an effective reward signal for training an off-the-shelf RL algorithm.</p>
</body>

</html>